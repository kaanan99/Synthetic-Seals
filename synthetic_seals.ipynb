{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the GPU if one exists.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using: \", device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seed = 42\n",
    "random.seed(my_seed)\n",
    "torch.manual_seed(my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./rcnn_training_data_transformation_True_step_50_sub_image_size_150.pkl\", \"rb\") as f:\n",
    "    seal_sub_images = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image_list, num_images, image_size):\n",
    "    fig, ax = plt.subplots(ncols=num_images, figsize=(image_size, image_size))\n",
    "    for idx in range(num_images):\n",
    "        image = image_list[idx]\n",
    "        ax[idx].imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out Sub-images below a certain threshold\n",
    "seal_threshold=.6\n",
    "filtered_seal_sub_images = []\n",
    "\n",
    "for image_name in seal_sub_images.keys():\n",
    "    \n",
    "    # Extract sub-images anb bounding box information\n",
    "    sub_image_list, bouding_box_list = seal_sub_images[image_name]\n",
    "    \n",
    "    # Filter out sub-images\n",
    "    for idx in range(len(sub_image_list)):\n",
    "\n",
    "        # Individual bounding box and sub-image\n",
    "        bounding_box_data_frame = bouding_box_list[idx]\n",
    "        sub_image = sub_image_list[idx]\n",
    "\n",
    "        # Only keep sub-images where atleast one seal is greater than the threshold\n",
    "        if bounding_box_data_frame.percent.max() >= seal_threshold:\n",
    "            filtered_seal_sub_images.append(sub_image)\n",
    "\n",
    "\n",
    "print(f\"Sub-images used for training: {len(filtered_seal_sub_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(filtered_seal_sub_images, 4, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for DCGAN \n",
    "batch_size = 100\n",
    "image_size = 150\n",
    "channnels = 3\n",
    "latent_space = 256\n",
    "generator_features = 64\n",
    "discriminator_features = 64\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_weights_normal_dist(layer):\n",
    "    layer_type = layer.__class__.__name__\n",
    "    # Layer is convolutional \n",
    "    if layer_type.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(layer.weight.data, 0, .02)\n",
    "    elif layer_type.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(layer.weight.data, 1, .02)\n",
    "        nn.init.constant(layer.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels, latent_space, image_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = (image_size // 16) + 1\n",
    "\n",
    "        self.initial_layer = nn.Sequential(\n",
    "            nn.Linear(latent_space, latent_space * self.init_size ** 2)\n",
    "        )\n",
    "\n",
    "        self.convolutional_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(latent_space),\n",
    "\n",
    "            # Block 1\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(latent_space, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128, .8),\n",
    "            nn.ReLU(.2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64, .8),\n",
    "            nn.ReLU(.2),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 32, 3, 1, 1),\n",
    "            nn.BatchNorm2d(32, .8),\n",
    "            nn.ReLU(.2),\n",
    "\n",
    "            # Block 4\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(32, 16, 3, 1, 1),\n",
    "            nn.BatchNorm2d(16, .8),\n",
    "            nn.ReLU(.2),\n",
    "\n",
    "            nn.Conv2d(16, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        noise_imgs = self.initial_layer(input)\n",
    "\n",
    "        # Reshape \n",
    "        noise_imgs = noise_imgs.view(noise_imgs.shape[0], latent_space, self.init_size, self.init_size)\n",
    "\n",
    "        # Generate Images\n",
    "        generated_imgs = self.convolutional_blocks(noise_imgs)\n",
    "\n",
    "        # Shave off 10 pixels from the end so the image size is 150x150\n",
    "        return generated_imgs[:, :, :-10, :-10]\n",
    "    \n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channnels, image_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            block = [\n",
    "                nn.Conv2d(in_filters, out_filters, 3, 2, 1),\n",
    "                nn.LeakyReLU(.2, inplace=True),\n",
    "                nn.Dropout2d(.4),\n",
    "            ]\n",
    "\n",
    "            if normalization:\n",
    "                block.append(nn.BatchNorm2d(out_filters, .8))\n",
    "            \n",
    "            return block\n",
    "                \n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channnels, 16, False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout2d(.4),\n",
    "            nn.Linear(12800, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SealDataset(Dataset):\n",
    "    def __init__(self, images, transform=transforms.Compose([transforms.ToTensor()])):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        self.length = len(self.images)\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        return self.transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Loader\n",
    "training_data = SealDataset(filtered_seal_sub_images)\n",
    "training_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models, loss, and optimizers\n",
    "gan_loss = torch.nn.MSELoss().to(device)\n",
    "\n",
    "generator = Generator(channnels, generator_features, latent_space, 150).to(device)\n",
    "discriminator = Discriminator(channnels, image_size).to(device)\n",
    "\n",
    "generator.apply(initial_weights_normal_dist)\n",
    "discriminator.apply(initial_weights_normal_dist)\n",
    "\n",
    "generator_opt = torch.optim.Adam(generator.parameters(), lr=.0001)\n",
    "discriminator_opt = torch.optim.Adam(discriminator.parameters(), lr=.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "model_name = \"seal_gan_msse_loss\"\n",
    "directory_name = f\"{model_name}_generated_images\"\n",
    "\n",
    "if directory_name not in os.listdir():\n",
    "    os.makedirs(f\"./{directory_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for batch in tqdm(training_loader, desc=f\"Epoch: {epoch + 1}\"):\n",
    "        '''\n",
    "        ---------------------------------------------------------------\n",
    "        Initialize Data\n",
    "        ---------------------------------------------------------------\n",
    "        '''\n",
    "\n",
    "        real_images = batch.to(device)\n",
    "        noise = torch.randn((batch_size, latent_space)).to(device)\n",
    "        generated_images = generator(noise)\n",
    "\n",
    "        '''\n",
    "        ---------------------------------------------------------------\n",
    "        Train Discriminator\n",
    "        ---------------------------------------------------------------\n",
    "        '''\n",
    "        \n",
    "        discriminator_opt.zero_grad()\n",
    "\n",
    "        # Get predictions on Real\n",
    "        real_predictions = discriminator(real_images)\n",
    "        real_labels = torch.ones(real_predictions.shape).to(device)\n",
    "\n",
    "        # Get predictions on fake\n",
    "        fake_predictions = discriminator(generated_images)\n",
    "        fake_labels = torch.zeros(fake_predictions.shape).to(device)\n",
    "    \n",
    "        # Combine\n",
    "        total_predictions = torch.cat((real_predictions, fake_predictions))\n",
    "        total_labels = torch.cat((real_labels, fake_labels))\n",
    "\n",
    "        # Backprop\n",
    "        discriminator_loss = gan_loss(total_predictions, total_labels)\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_opt.step()\n",
    "\n",
    "        '''\n",
    "        ---------------------------------------------------------------\n",
    "        Train Generator\n",
    "        ---------------------------------------------------------------\n",
    "        '''\n",
    "\n",
    "        generator_opt.zero_grad()\n",
    "        \n",
    "        # 0 for discriminator is 1 for generator\n",
    "        generator_labels = torch.ones(fake_predictions.shape, requires_grad=True).to(device)\n",
    "        fake_predictions = discriminator(generated_images).detach()\n",
    "        generator_loss = gan_loss(fake_predictions, generator_labels)\n",
    "        generator_loss.backward()\n",
    "        generator_opt.step() \n",
    "\n",
    "    # Display and save loss information\n",
    "    print(f\"Discriminator Loss: {discriminator_loss} Generator Loss: {generator_loss}\")\n",
    "    discriminator_losses.append(discriminator_loss)\n",
    "    generator_losses.append(generator_loss)\n",
    "\n",
    "    # Save model\n",
    "    if epoch + 1 in checkpoints:\n",
    "        # Save Generator\n",
    "        torch.save(\n",
    "            generator.state_dict(),\n",
    "            f\"./Models/generator_{model_name}_epoch_{epoch+1}\"\n",
    "        )\n",
    "\n",
    "        # Save Discrisminator\n",
    "        torch.save(\n",
    "            discriminator.state_dict(),\n",
    "            f\"./Models/discriminator_{model_name}_epoch_{epoch+1}\"\n",
    "        )\n",
    "\n",
    "    # Generate Sample images\n",
    "    with torch.no_grad():\n",
    "        fake_samples = generator(torch.randn(64, latent_space).to(device))\n",
    "        fake_samples = fake_samples.view(fake_samples.size(0), channnels, image_size, image_size)\n",
    "        save_image(fake_samples, f\"{directory_name}/generated_images_epoch_{epoch+1}.png\", normalize=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthetic_seals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
